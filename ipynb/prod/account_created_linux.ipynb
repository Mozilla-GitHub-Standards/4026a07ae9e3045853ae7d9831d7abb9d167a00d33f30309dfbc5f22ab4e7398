{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Retention using account.created events Linux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date, timedelta, datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "# Only initialize Spark if testing locally\n",
    "# Otherwise it should be already running within Spark\n",
    "try:\n",
    "    from pyspark import SparkContext\n",
    "except ImportError:\n",
    "    import spark_env\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext, Row\n",
    "\n",
    "IN_IPYTHON = True\n",
    "\n",
    "try:\n",
    "    __IPYTHON__\n",
    "except NameError:\n",
    "    IN_IPYTHON = False\n",
    "    sc = SparkContext('local')\n",
    "    print \"Not in IPython, creating SparkContext manually\"\n",
    "\n",
    "\n",
    "def week_file(week):\n",
    "    event_storage = os.path.join('s3n://net-mozaws-prod-us-west-2-pipeline-analysis/fxa-retention/data/', 'events-' + week + '.csv')\n",
    "\n",
    "    # if not ipython that probably means you are not running this on a Spark Cluster\n",
    "    # the telemetry spark cluster only supports uploading a ipynb.\n",
    "    if not IN_IPYTHON:\n",
    "        print 'Failed to find:' + event_storage\n",
    "        event_storage = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', 'tools', 'out', 'events-' + week + '.csv')\n",
    "    return event_storage\n",
    "\n",
    "# sc will be global in IPython\n",
    "sqlContext = SQLContext(sc)\n",
    "#today = date.today()\n",
    "today = datetime.strptime('2015-09-28', '%Y-%m-%d').date()\n",
    "last_monday = today - timedelta(days=-today.weekday(), weeks=1)\n",
    "week_range = pd.date_range(end=last_monday, periods=12, freq='W-MON')\n",
    "\n",
    "# TODO for now: from events-2015-06-15.csv to events-2015-09-21.csv\n",
    "WEEKS = week_range.map(lambda x: x.strftime('%Y-%m-%d'))\n",
    "\n",
    "out_data = []\n",
    "for x in range(0, len(WEEKS)):\n",
    "    out_data.append([0] * len(WEEKS))\n",
    "\n",
    "for x in range(0, len(WEEKS)):\n",
    "    saved_uids = None\n",
    "    saved_uids_count = None\n",
    "\n",
    "    idx = 0\n",
    "    for week in WEEKS[x:]:\n",
    "        df = sqlContext.load(source='com.databricks.spark.csv', header='false', path=week_file(week))\n",
    "        table_name = 'week' + week.replace('-', '_')\n",
    "        df.registerTempTable(table_name)\n",
    "\n",
    "        if not saved_uids:\n",
    "            # TODO: there are no csv headers, so have to use index based columns\n",
    "            signed_events = sqlContext.sql(\"SELECT C4 FROM \" + table_name + \" WHERE C5 = 'account.created' AND C3 = 'Linux'\")\n",
    "            new_uids = signed_events.map(lambda p: p.C4).distinct()\n",
    "\n",
    "            saved_uids = new_uids\n",
    "            saved_uids_count = int(new_uids.count())\n",
    "            out_data[x][idx] = 100\n",
    "        else:\n",
    "            created_events = sqlContext.sql(\"SELECT C4 FROM \" + table_name + \" WHERE C5 = 'account.signed' AND C3 = 'Linux'\")\n",
    "            new_uids_created_events = created_events.map(lambda p: p.C4).distinct()\n",
    "\n",
    "            retention_uids = saved_uids.intersection(new_uids_created_events)\n",
    "            if saved_uids_count > 0:\n",
    "                percentage = int((float(retention_uids.count()) / float(saved_uids_count)) * 100)\n",
    "            else:\n",
    "                percentage = 0\n",
    "            out_data[x][idx] = percentage\n",
    "        idx += 1\n",
    "\n",
    "df = pd.DataFrame(out_data, index=week_range, columns=range(0, 12))\n",
    "\n",
    "if IN_IPYTHON:\n",
    "    seaborn.set(style='white')\n",
    "    plt.figure(figsize=(14, 12))\n",
    "    plt.title('User Retention MAC based on \"account.created\" and then \"account.signed\"')\n",
    "    seaborn.heatmap(df, annot=True, fmt='d', yticklabels=week_range, xticklabels=range(0, 12))\n",
    "    # Rotate labels\n",
    "    locs, labels = plt.yticks()\n",
    "    plt.setp(labels, rotation=0)\n",
    "    # Set axis font\n",
    "    font = {\n",
    "        'weight': 'bold',\n",
    "        'size': 22\n",
    "    }\n",
    "    # Label axis\n",
    "    plt.ylabel('Starting Week', **font)\n",
    "    plt.xlabel('Retention Weeks', **font)\n",
    "else:\n",
    "    print df\n"
   ]
  }
 ],
 "metadata": {
  "language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
